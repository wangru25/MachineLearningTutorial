{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "e70076c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "216ca96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(feature_file, label_file):\n",
    "    '''Read dataset in *.csv to dataframe in pandas'''\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values # convert values in dataframe to numpy array (feature)\n",
    "    y = df_y.values # convert values in dataframe to numpy array (label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "6aff6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler # import library \n",
    "    scaler = StandardScaler() # call an object function\n",
    "    scaler.fit(X_train)   # calculate mean, std in X_train  (x-u)/s\n",
    "    X_train_norm = scaler.transform(X_train)  # apply normalization on X_train\n",
    "    X_test_norm = scaler.transform(X_test)    # apply normalization on X_test\n",
    "    return X_train_norm, X_test_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "34c8bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcc(y_pred, y_test):\n",
    "   '''Pearson correlation coefficient measures the linear relationship between two datasets. '''\n",
    "   from scipy import stats\n",
    "   a = y_test.ravel()\n",
    "   b = y_pred.ravel()\n",
    "   pcc = stats.pearsonr(a,b)[0]\n",
    "   return pcc\n",
    "\n",
    "def Accuracy(ypred, yexact):\n",
    "   p = np.array(ypred == yexact, dtype = int)\n",
    "   return np.sum(p)/float(len(yexact)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "1b3c46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(ypred, yexact):\n",
    "    '''Root Mean Square Error'''\n",
    "    return np.sqrt(np.sum((ypred - yexact)**2)/ ypred.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e89f6",
   "metadata": {},
   "source": [
    "^ Definition of important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "f69dedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_dataset('Iris_X_train.csv', 'Iris_y_train.csv')\n",
    "X_test, y_test = read_dataset('Iris_X_test.csv', 'Iris_y_test.csv')\n",
    "X_train_norm, X_test_norm = normalize_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d342d",
   "metadata": {},
   "source": [
    "^ Importing Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "b02e9f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2)\n",
      "(38, 2)\n",
      "(112, 1)\n",
      "(38, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_norm.shape)\n",
    "print(X_test_norm.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1c7b4",
   "metadata": {},
   "source": [
    "Let $N$ denote the number of elements, $D$ the number of features. Then $X$ has shape $(N,D)$, $W$ has shape $(D,1)$, and we have:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y}  &= \\frac{1}{1-e^{-c^Tx}}\\\\ \n",
    "    \\text{let } z   &= c^Tx \\\\\n",
    "                   &= c_0 + c_1x_1 + c_2x_2 \\\\\n",
    "    \\hat{y}      &= (1-e^{-z})^{-1} \\\\\n",
    "    L &= -\\frac{1}{M }\\sum_{i = 1}^{M} (y^{(i)}\\log(\\hat{y}) + (1-y^{(i)})\\log(1-\\hat{y}) \\\\\n",
    "    %\\dfrac{\\partial{L}}{\\partial{W}}  &= \\dfrac{\\partial{L}}{\\partial{\\hat{y}}} \\dfrac{\\partial{\\hat{y}}}{\\partial{W}} = \\frac{1}{N} X^T (\\hat{y}-y))\\\\\n",
    "    %\\dfrac{\\partial{L}}{\\partial{b}}  &= \\dfrac{\\partial{L}}{\\partial{\\hat{y}}} \\dfrac{\\partial{\\hat{y}}}{\\partial{b}} = \\sum_{i}\\frac{1}{N}(\\hat{y}_i-y_i) \\\\\n",
    "  %  W                                 &:= W - lr*\\dfrac{\\partial{L}}{\\partial{W}}\\\\\n",
    "  %  b                                 &:= b - lr *\\dfrac{\\partial{L}}{\\partial{b}}\n",
    "\\end{align}\n",
    "\n",
    "Then, we need to calculate the partial derivatives with respect to $c_0, c_1$, and $c_2$. Notice that \n",
    "$$ \\dfrac{\\partial{z}}{\\partial{c_i}} = x_i$$ where $x_0 = 0$.\n",
    "Then, calculating \n",
    "\n",
    "\\begin{align}\n",
    "    \\dfrac{\\partial{\\hat{y}}}{\\partial{z}}  &= (-1)(1+e^{-z})^{-2})e^{-z}(-1) \\\\\n",
    "    &= (1+e^{-z})^{-2})e^{-z} \\\\\n",
    "    &= \\frac{e^{-z}}{(1+e^{-z})^2} \\\\\n",
    "    &= \\frac{e^{-z}}{(1+e^{-z})} * \\frac{1}{(1+e^{-z})}\\\\\n",
    "    &= \\frac{e^{-z}}{(1+e^{-z})} * \\hat{y} \\\\\n",
    "    &= \\frac{-1+1+ e^{-z}}{1+e^{-z}} * \\hat{y} \\\\\n",
    "    &=(\\frac{1}{1+e^{-z}} + \\frac{-(1 +e^{-z})}{1+e^{-z}} )* \\hat{y} \\\\\n",
    "    &= (\\hat{y} - 1)\\hat{y}\\\\\n",
    "\\end{align}\n",
    "\n",
    "And using that, we can find\n",
    "\\begin{align}\n",
    "     \\dfrac{\\partial{L}}{\\partial{c_i}} &= -\\sum_{i=1}^{M} (\\frac{y^{(i)}}{\\hat{y}} \\cdot \\dfrac{\\partial{\\hat{y}}}{\\partial{z}} \\cdot  \\dfrac{\\partial{z}}{\\partial{c_i}} - \\frac{1-y^{(i)}}{1-\\hat{y}} \\cdot  \\dfrac{\\partial{\\hat{y}}}{\\partial{z}} \\cdot  \\dfrac{\\partial{z}}{\\partial{c_i}} ) \\\\\n",
    "     &= -\\sum_{i=1}^{M}  \\dfrac{\\partial{z}}{\\partial{c_i}} (\\frac{y^{(i)}}{\\hat{y}} \\cdot \\dfrac{\\partial{\\hat{y}}}{\\partial{z}}  - \\frac{1-y^{(i)}}{1-\\hat{y}} \\cdot  \\dfrac{\\partial{\\hat{y}}}{\\partial{z}}) \\\\\n",
    "     &= -\\sum_{i=1}^{M}  \\dfrac{\\partial{z}}{\\partial{c_i}} (\\frac{y^{(i)}}{\\hat{y}} \\cdot (\\hat{y} - 1)\\hat{y}  - \\frac{1-y^{(i)}}{1-\\hat{y}} \\cdot  (\\hat{y} - 1)\\hat{y}) \\\\\n",
    "      &= -\\sum_{i=1}^{M}  \\dfrac{\\partial{z}}{\\partial{c_i}} (y^{(i)}(1-\\hat{y})-(1-y^{(i)})\\hat{y}) \\\\\n",
    "     &= -\\sum_{i=1}^{M}  \\dfrac{\\partial{z}}{\\partial{c_i}} (y^{(i)}-\\hat{y})\n",
    "\\end{align}\n",
    "\n",
    "Then\n",
    "\\begin{align}\n",
    "    \\dfrac{\\partial{L}}{\\partial{c_1}} &= -\\sum_{i=1}^{M}  \\dfrac{\\partial{z}}{\\partial{c_i}} (y^{(i)}-\\hat{y}) \\\\\n",
    "                                      &= -\\sum_{i=1}^M   0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "82219fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self, X, y, lr = 0.01):\n",
    "        self.X1 = X[:,0]\n",
    "        self.X2 = X[:,1]\n",
    "        self.y = y\n",
    "        self.lr = lr\n",
    "        self.c0 = 0\n",
    "        self.c1 = 0\n",
    "        self.c2 = 0\n",
    "\n",
    "    def forward(self):\n",
    "        self.y_hat = 1.0 / (1+np.exp(-(self.c0 +(self.c1 * (self.X1) ) + (self.c2 * (self.X2)) ))).reshape(-1,1)\n",
    "        # print(self.y_hat.shape)\n",
    "        # print(self.y.shape)\n",
    "        \n",
    "    def gradientDescent(self):\n",
    "        d = (self.y_hat-self.y) / self.X1.shape[0]\n",
    "        dc1 = np.sum(d * (self.X1).T)\n",
    "        dc0 = np.sum(d)\n",
    "        dc2 = np.sum(d * (self.X2).T)\n",
    "        self.c0 = self.c0 - self.lr * dc0\n",
    "        self.c1 = self.c1 - self.lr * dc1\n",
    "        self.c2 = self.c2 - self.lr * dc2\n",
    "        \n",
    "    def lossfunction(self):\n",
    "        self.forward()\n",
    "        self.loss =  -(1/self.X1.shape[0]) * np.sum((self.y) * np.log(self.y_hat) + (1-self.y) * np.log(1-self.y_hat))\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        z = self.c0 + (self.c1 * (X_test[:,0]) ) + (self.c2 * (X_test[:,1]) )\n",
    "        y_hat_test = 1.0 / (1+np.exp(-z)).ravel()\n",
    "\n",
    "        ypred = np.zeros(X_test.shape[0],  dtype=int)\n",
    "        for i in range(X_test.shape[0]):\n",
    "            if y_hat_test[i] >= 0.5:\n",
    "                ypred[i] = 1\n",
    "            else:\n",
    "                ypred[i] = 0\n",
    "        return ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "5600832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myLR = Logistic_Regression(X_train_norm, y_train, 0.1)\n",
    "# myLR.forward()\n",
    "# myLR.gradientDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "9cdb9052",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(myLR.c0)\n",
    "# print(myLR.c1)\n",
    "# print(myLR.c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "4c437b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5, current loss = 0.69201\n",
      "epoch = 10, current loss = 0.69090\n",
      "epoch = 15, current loss = 0.68981\n",
      "epoch = 20, current loss = 0.68876\n",
      "epoch = 25, current loss = 0.68773\n",
      "epoch = 30, current loss = 0.68672\n",
      "epoch = 35, current loss = 0.68574\n",
      "epoch = 40, current loss = 0.68479\n",
      "epoch = 45, current loss = 0.68385\n",
      "epoch = 50, current loss = 0.68294\n",
      "epoch = 55, current loss = 0.68206\n",
      "epoch = 60, current loss = 0.68119\n",
      "epoch = 65, current loss = 0.68035\n",
      "epoch = 70, current loss = 0.67952\n",
      "epoch = 75, current loss = 0.67872\n",
      "epoch = 80, current loss = 0.67794\n",
      "epoch = 85, current loss = 0.67718\n",
      "epoch = 90, current loss = 0.67643\n",
      "epoch = 95, current loss = 0.67570\n",
      "epoch = 100, current loss = 0.67500\n",
      "epoch = 105, current loss = 0.67431\n",
      "epoch = 110, current loss = 0.67363\n",
      "epoch = 115, current loss = 0.67297\n",
      "epoch = 120, current loss = 0.67233\n",
      "epoch = 125, current loss = 0.67171\n",
      "epoch = 130, current loss = 0.67110\n",
      "epoch = 135, current loss = 0.67050\n",
      "epoch = 140, current loss = 0.66992\n",
      "epoch = 145, current loss = 0.66935\n",
      "epoch = 150, current loss = 0.66880\n",
      "epoch = 155, current loss = 0.66826\n",
      "epoch = 160, current loss = 0.66774\n",
      "epoch = 165, current loss = 0.66722\n",
      "epoch = 170, current loss = 0.66672\n",
      "epoch = 175, current loss = 0.66624\n",
      "epoch = 180, current loss = 0.66576\n",
      "epoch = 185, current loss = 0.66530\n",
      "epoch = 190, current loss = 0.66484\n",
      "epoch = 195, current loss = 0.66440\n",
      "epoch = 200, current loss = 0.66397\n",
      "epoch = 205, current loss = 0.66355\n",
      "epoch = 210, current loss = 0.66314\n",
      "epoch = 215, current loss = 0.66274\n",
      "epoch = 220, current loss = 0.66235\n",
      "epoch = 225, current loss = 0.66196\n",
      "epoch = 230, current loss = 0.66159\n",
      "epoch = 235, current loss = 0.66123\n",
      "epoch = 240, current loss = 0.66088\n",
      "epoch = 245, current loss = 0.66053\n",
      "epoch = 250, current loss = 0.66019\n",
      "epoch = 255, current loss = 0.65986\n",
      "epoch = 260, current loss = 0.65954\n",
      "epoch = 265, current loss = 0.65923\n",
      "epoch = 270, current loss = 0.65892\n",
      "epoch = 275, current loss = 0.65862\n",
      "epoch = 280, current loss = 0.65833\n",
      "epoch = 285, current loss = 0.65805\n",
      "epoch = 290, current loss = 0.65777\n",
      "epoch = 295, current loss = 0.65750\n",
      "epoch = 300, current loss = 0.65724\n",
      "epoch = 305, current loss = 0.65698\n",
      "epoch = 310, current loss = 0.65673\n",
      "epoch = 315, current loss = 0.65648\n",
      "epoch = 320, current loss = 0.65624\n",
      "epoch = 325, current loss = 0.65601\n",
      "epoch = 330, current loss = 0.65578\n",
      "epoch = 335, current loss = 0.65556\n",
      "epoch = 340, current loss = 0.65534\n",
      "epoch = 345, current loss = 0.65513\n",
      "epoch = 350, current loss = 0.65492\n",
      "epoch = 355, current loss = 0.65472\n",
      "epoch = 360, current loss = 0.65452\n",
      "epoch = 365, current loss = 0.65433\n",
      "epoch = 370, current loss = 0.65414\n",
      "epoch = 375, current loss = 0.65396\n",
      "epoch = 380, current loss = 0.65378\n",
      "epoch = 385, current loss = 0.65361\n",
      "epoch = 390, current loss = 0.65344\n",
      "epoch = 395, current loss = 0.65327\n",
      "epoch = 400, current loss = 0.65311\n",
      "epoch = 405, current loss = 0.65295\n",
      "epoch = 410, current loss = 0.65279\n",
      "epoch = 415, current loss = 0.65264\n",
      "epoch = 420, current loss = 0.65249\n",
      "epoch = 425, current loss = 0.65235\n",
      "epoch = 430, current loss = 0.65221\n",
      "epoch = 435, current loss = 0.65207\n",
      "epoch = 440, current loss = 0.65194\n",
      "epoch = 445, current loss = 0.65181\n",
      "epoch = 450, current loss = 0.65168\n",
      "epoch = 455, current loss = 0.65156\n",
      "epoch = 460, current loss = 0.65143\n",
      "epoch = 465, current loss = 0.65131\n",
      "epoch = 470, current loss = 0.65120\n",
      "epoch = 475, current loss = 0.65109\n",
      "epoch = 480, current loss = 0.65097\n",
      "epoch = 485, current loss = 0.65087\n",
      "epoch = 490, current loss = 0.65076\n",
      "epoch = 495, current loss = 0.65066\n",
      "epoch = 500, current loss = 0.65056\n",
      "epoch = 505, current loss = 0.65046\n",
      "epoch = 510, current loss = 0.65036\n",
      "epoch = 515, current loss = 0.65027\n",
      "epoch = 520, current loss = 0.65018\n",
      "epoch = 525, current loss = 0.65009\n",
      "epoch = 530, current loss = 0.65000\n",
      "epoch = 535, current loss = 0.64992\n",
      "epoch = 540, current loss = 0.64983\n",
      "epoch = 545, current loss = 0.64975\n",
      "epoch = 550, current loss = 0.64967\n",
      "epoch = 555, current loss = 0.64960\n",
      "epoch = 560, current loss = 0.64952\n",
      "epoch = 565, current loss = 0.64945\n",
      "epoch = 570, current loss = 0.64937\n",
      "epoch = 575, current loss = 0.64930\n",
      "epoch = 580, current loss = 0.64924\n",
      "epoch = 585, current loss = 0.64917\n",
      "epoch = 590, current loss = 0.64910\n",
      "epoch = 595, current loss = 0.64904\n",
      "epoch = 600, current loss = 0.64898\n",
      "epoch = 605, current loss = 0.64892\n",
      "epoch = 610, current loss = 0.64886\n",
      "epoch = 615, current loss = 0.64880\n",
      "epoch = 620, current loss = 0.64874\n",
      "epoch = 625, current loss = 0.64869\n",
      "epoch = 630, current loss = 0.64863\n",
      "epoch = 635, current loss = 0.64858\n",
      "epoch = 640, current loss = 0.64853\n",
      "epoch = 645, current loss = 0.64848\n",
      "epoch = 650, current loss = 0.64843\n",
      "epoch = 655, current loss = 0.64838\n",
      "epoch = 660, current loss = 0.64833\n",
      "epoch = 665, current loss = 0.64829\n",
      "epoch = 670, current loss = 0.64824\n",
      "epoch = 675, current loss = 0.64820\n",
      "epoch = 680, current loss = 0.64815\n",
      "epoch = 685, current loss = 0.64811\n",
      "epoch = 690, current loss = 0.64807\n",
      "epoch = 695, current loss = 0.64803\n",
      "epoch = 700, current loss = 0.64799\n",
      "epoch = 705, current loss = 0.64795\n",
      "epoch = 710, current loss = 0.64792\n",
      "epoch = 715, current loss = 0.64788\n",
      "epoch = 720, current loss = 0.64785\n",
      "epoch = 725, current loss = 0.64781\n",
      "epoch = 730, current loss = 0.64778\n",
      "epoch = 735, current loss = 0.64774\n",
      "epoch = 740, current loss = 0.64771\n",
      "epoch = 745, current loss = 0.64768\n",
      "epoch = 750, current loss = 0.64765\n",
      "epoch = 755, current loss = 0.64762\n",
      "epoch = 760, current loss = 0.64759\n",
      "epoch = 765, current loss = 0.64756\n",
      "epoch = 770, current loss = 0.64753\n",
      "epoch = 775, current loss = 0.64750\n",
      "epoch = 780, current loss = 0.64748\n",
      "epoch = 785, current loss = 0.64745\n",
      "epoch = 790, current loss = 0.64743\n",
      "epoch = 795, current loss = 0.64740\n",
      "epoch = 800, current loss = 0.64738\n",
      "epoch = 805, current loss = 0.64735\n",
      "epoch = 810, current loss = 0.64733\n",
      "epoch = 815, current loss = 0.64731\n",
      "epoch = 820, current loss = 0.64728\n",
      "epoch = 825, current loss = 0.64726\n",
      "epoch = 830, current loss = 0.64724\n",
      "epoch = 835, current loss = 0.64722\n",
      "epoch = 840, current loss = 0.64720\n",
      "epoch = 845, current loss = 0.64718\n",
      "epoch = 850, current loss = 0.64716\n",
      "epoch = 855, current loss = 0.64714\n",
      "epoch = 860, current loss = 0.64712\n",
      "epoch = 865, current loss = 0.64711\n",
      "epoch = 870, current loss = 0.64709\n",
      "epoch = 875, current loss = 0.64707\n",
      "epoch = 880, current loss = 0.64705\n",
      "epoch = 885, current loss = 0.64704\n",
      "epoch = 890, current loss = 0.64702\n",
      "epoch = 895, current loss = 0.64701\n",
      "epoch = 900, current loss = 0.64699\n",
      "epoch = 905, current loss = 0.64698\n",
      "epoch = 910, current loss = 0.64696\n",
      "epoch = 915, current loss = 0.64695\n",
      "epoch = 920, current loss = 0.64693\n",
      "epoch = 925, current loss = 0.64692\n",
      "epoch = 930, current loss = 0.64691\n",
      "epoch = 935, current loss = 0.64689\n",
      "epoch = 940, current loss = 0.64688\n",
      "epoch = 945, current loss = 0.64687\n",
      "epoch = 950, current loss = 0.64686\n",
      "epoch = 955, current loss = 0.64684\n",
      "epoch = 960, current loss = 0.64683\n",
      "epoch = 965, current loss = 0.64682\n",
      "epoch = 970, current loss = 0.64681\n",
      "epoch = 975, current loss = 0.64680\n",
      "epoch = 980, current loss = 0.64679\n",
      "epoch = 985, current loss = 0.64678\n",
      "epoch = 990, current loss = 0.64677\n",
      "epoch = 995, current loss = 0.64676\n",
      "epoch = 1000, current loss = 0.64675\n",
      "epoch = 1005, current loss = 0.64674\n",
      "epoch = 1010, current loss = 0.64673\n",
      "epoch = 1015, current loss = 0.64672\n",
      "epoch = 1020, current loss = 0.64671\n",
      "epoch = 1025, current loss = 0.64670\n",
      "epoch = 1030, current loss = 0.64670\n",
      "epoch = 1035, current loss = 0.64669\n",
      "epoch = 1040, current loss = 0.64668\n",
      "epoch = 1045, current loss = 0.64667\n",
      "epoch = 1050, current loss = 0.64666\n",
      "epoch = 1055, current loss = 0.64666\n",
      "epoch = 1060, current loss = 0.64665\n",
      "epoch = 1065, current loss = 0.64664\n",
      "epoch = 1070, current loss = 0.64663\n",
      "epoch = 1075, current loss = 0.64663\n",
      "epoch = 1080, current loss = 0.64662\n",
      "epoch = 1085, current loss = 0.64661\n",
      "epoch = 1090, current loss = 0.64661\n",
      "epoch = 1095, current loss = 0.64660\n",
      "epoch = 1100, current loss = 0.64660\n",
      "epoch = 1105, current loss = 0.64659\n",
      "epoch = 1110, current loss = 0.64658\n",
      "epoch = 1115, current loss = 0.64658\n",
      "epoch = 1120, current loss = 0.64657\n",
      "epoch = 1125, current loss = 0.64657\n",
      "epoch = 1130, current loss = 0.64656\n",
      "epoch = 1135, current loss = 0.64656\n",
      "epoch = 1140, current loss = 0.64655\n",
      "epoch = 1145, current loss = 0.64655\n",
      "epoch = 1150, current loss = 0.64654\n",
      "epoch = 1155, current loss = 0.64654\n",
      "epoch = 1160, current loss = 0.64653\n",
      "epoch = 1165, current loss = 0.64653\n",
      "epoch = 1170, current loss = 0.64652\n",
      "epoch = 1175, current loss = 0.64652\n",
      "epoch = 1180, current loss = 0.64652\n",
      "epoch = 1185, current loss = 0.64651\n",
      "epoch = 1190, current loss = 0.64651\n",
      "epoch = 1195, current loss = 0.64650\n",
      "epoch = 1200, current loss = 0.64650\n",
      "epoch = 1205, current loss = 0.64650\n",
      "epoch = 1210, current loss = 0.64649\n",
      "epoch = 1215, current loss = 0.64649\n",
      "epoch = 1220, current loss = 0.64649\n",
      "epoch = 1225, current loss = 0.64648\n",
      "epoch = 1230, current loss = 0.64648\n",
      "epoch = 1235, current loss = 0.64648\n",
      "epoch = 1240, current loss = 0.64647\n",
      "epoch = 1245, current loss = 0.64647\n",
      "epoch = 1250, current loss = 0.64647\n",
      "epoch = 1255, current loss = 0.64646\n",
      "epoch = 1260, current loss = 0.64646\n",
      "epoch = 1265, current loss = 0.64646\n",
      "epoch = 1270, current loss = 0.64645\n",
      "epoch = 1275, current loss = 0.64645\n",
      "epoch = 1280, current loss = 0.64645\n",
      "epoch = 1285, current loss = 0.64645\n",
      "epoch = 1290, current loss = 0.64644\n",
      "epoch = 1295, current loss = 0.64644\n",
      "epoch = 1300, current loss = 0.64644\n",
      "epoch = 1305, current loss = 0.64644\n",
      "epoch = 1310, current loss = 0.64643\n",
      "epoch = 1315, current loss = 0.64643\n",
      "epoch = 1320, current loss = 0.64643\n",
      "epoch = 1325, current loss = 0.64643\n",
      "epoch = 1330, current loss = 0.64643\n",
      "epoch = 1335, current loss = 0.64642\n",
      "epoch = 1340, current loss = 0.64642\n",
      "epoch = 1345, current loss = 0.64642\n",
      "epoch = 1350, current loss = 0.64642\n",
      "epoch = 1355, current loss = 0.64642\n",
      "epoch = 1360, current loss = 0.64641\n",
      "epoch = 1365, current loss = 0.64641\n",
      "epoch = 1370, current loss = 0.64641\n",
      "epoch = 1375, current loss = 0.64641\n",
      "epoch = 1380, current loss = 0.64641\n",
      "epoch = 1385, current loss = 0.64641\n",
      "epoch = 1390, current loss = 0.64640\n",
      "epoch = 1395, current loss = 0.64640\n",
      "epoch = 1400, current loss = 0.64640\n",
      "epoch = 1405, current loss = 0.64640\n",
      "epoch = 1410, current loss = 0.64640\n",
      "epoch = 1415, current loss = 0.64640\n",
      "epoch = 1420, current loss = 0.64639\n",
      "epoch = 1425, current loss = 0.64639\n",
      "epoch = 1430, current loss = 0.64639\n",
      "epoch = 1435, current loss = 0.64639\n",
      "epoch = 1440, current loss = 0.64639\n",
      "epoch = 1445, current loss = 0.64639\n",
      "epoch = 1450, current loss = 0.64639\n",
      "epoch = 1455, current loss = 0.64639\n",
      "epoch = 1460, current loss = 0.64638\n",
      "epoch = 1465, current loss = 0.64638\n",
      "epoch = 1470, current loss = 0.64638\n",
      "epoch = 1475, current loss = 0.64638\n",
      "epoch = 1480, current loss = 0.64638\n",
      "epoch = 1485, current loss = 0.64638\n",
      "epoch = 1490, current loss = 0.64638\n",
      "epoch = 1495, current loss = 0.64638\n",
      "epoch = 1500, current loss = 0.64638\n",
      "epoch = 1505, current loss = 0.64638\n",
      "epoch = 1510, current loss = 0.64637\n",
      "epoch = 1515, current loss = 0.64637\n",
      "epoch = 1520, current loss = 0.64637\n",
      "epoch = 1525, current loss = 0.64637\n",
      "epoch = 1530, current loss = 0.64637\n",
      "epoch = 1535, current loss = 0.64637\n",
      "epoch = 1540, current loss = 0.64637\n",
      "epoch = 1545, current loss = 0.64637\n",
      "epoch = 1550, current loss = 0.64637\n",
      "epoch = 1555, current loss = 0.64637\n",
      "epoch = 1560, current loss = 0.64637\n",
      "epoch = 1565, current loss = 0.64637\n",
      "epoch = 1570, current loss = 0.64636\n",
      "epoch = 1575, current loss = 0.64636\n",
      "epoch = 1580, current loss = 0.64636\n",
      "epoch = 1585, current loss = 0.64636\n",
      "epoch = 1590, current loss = 0.64636\n",
      "epoch = 1595, current loss = 0.64636\n",
      "epoch = 1600, current loss = 0.64636\n",
      "epoch = 1605, current loss = 0.64636\n",
      "epoch = 1610, current loss = 0.64636\n",
      "epoch = 1615, current loss = 0.64636\n",
      "epoch = 1620, current loss = 0.64636\n",
      "epoch = 1625, current loss = 0.64636\n",
      "epoch = 1630, current loss = 0.64636\n",
      "epoch = 1635, current loss = 0.64636\n",
      "epoch = 1640, current loss = 0.64636\n",
      "epoch = 1645, current loss = 0.64636\n",
      "epoch = 1650, current loss = 0.64636\n",
      "epoch = 1655, current loss = 0.64635\n",
      "epoch = 1660, current loss = 0.64635\n",
      "epoch = 1665, current loss = 0.64635\n",
      "epoch = 1670, current loss = 0.64635\n",
      "epoch = 1675, current loss = 0.64635\n",
      "epoch = 1680, current loss = 0.64635\n",
      "epoch = 1685, current loss = 0.64635\n",
      "epoch = 1690, current loss = 0.64635\n",
      "epoch = 1695, current loss = 0.64635\n",
      "epoch = 1700, current loss = 0.64635\n",
      "epoch = 1705, current loss = 0.64635\n",
      "epoch = 1710, current loss = 0.64635\n",
      "epoch = 1715, current loss = 0.64635\n",
      "epoch = 1720, current loss = 0.64635\n",
      "epoch = 1725, current loss = 0.64635\n",
      "epoch = 1730, current loss = 0.64635\n",
      "epoch = 1735, current loss = 0.64635\n",
      "epoch = 1740, current loss = 0.64635\n",
      "epoch = 1745, current loss = 0.64635\n",
      "epoch = 1750, current loss = 0.64635\n",
      "epoch = 1755, current loss = 0.64635\n",
      "epoch = 1760, current loss = 0.64635\n",
      "epoch = 1765, current loss = 0.64635\n",
      "epoch = 1770, current loss = 0.64635\n",
      "epoch = 1775, current loss = 0.64635\n",
      "epoch = 1780, current loss = 0.64635\n",
      "epoch = 1785, current loss = 0.64635\n",
      "epoch = 1790, current loss = 0.64635\n",
      "epoch = 1795, current loss = 0.64635\n",
      "epoch = 1800, current loss = 0.64634\n",
      "epoch = 1805, current loss = 0.64634\n",
      "epoch = 1810, current loss = 0.64634\n",
      "epoch = 1815, current loss = 0.64634\n",
      "epoch = 1820, current loss = 0.64634\n",
      "epoch = 1825, current loss = 0.64634\n",
      "epoch = 1830, current loss = 0.64634\n",
      "epoch = 1835, current loss = 0.64634\n",
      "epoch = 1840, current loss = 0.64634\n",
      "epoch = 1845, current loss = 0.64634\n",
      "epoch = 1850, current loss = 0.64634\n",
      "epoch = 1855, current loss = 0.64634\n",
      "epoch = 1860, current loss = 0.64634\n",
      "epoch = 1865, current loss = 0.64634\n",
      "epoch = 1870, current loss = 0.64634\n",
      "epoch = 1875, current loss = 0.64634\n",
      "epoch = 1880, current loss = 0.64634\n",
      "epoch = 1885, current loss = 0.64634\n",
      "epoch = 1890, current loss = 0.64634\n",
      "epoch = 1895, current loss = 0.64634\n",
      "epoch = 1900, current loss = 0.64634\n",
      "epoch = 1905, current loss = 0.64634\n",
      "epoch = 1910, current loss = 0.64634\n",
      "epoch = 1915, current loss = 0.64634\n",
      "epoch = 1920, current loss = 0.64634\n",
      "epoch = 1925, current loss = 0.64634\n",
      "epoch = 1930, current loss = 0.64634\n",
      "epoch = 1935, current loss = 0.64634\n",
      "epoch = 1940, current loss = 0.64634\n",
      "epoch = 1945, current loss = 0.64634\n",
      "epoch = 1950, current loss = 0.64634\n",
      "epoch = 1955, current loss = 0.64634\n",
      "epoch = 1960, current loss = 0.64634\n",
      "epoch = 1965, current loss = 0.64634\n",
      "epoch = 1970, current loss = 0.64634\n",
      "epoch = 1975, current loss = 0.64634\n",
      "epoch = 1980, current loss = 0.64634\n",
      "epoch = 1985, current loss = 0.64634\n",
      "epoch = 1990, current loss = 0.64634\n",
      "epoch = 1995, current loss = 0.64634\n",
      "epoch = 2000, current loss = 0.64634\n"
     ]
    }
   ],
   "source": [
    "myLR = Logistic_Regression(X_train_norm, y_train, 0.01)\n",
    "epoch_num = 2000\n",
    "for i in range(epoch_num):\n",
    "    myLR.forward()\n",
    "    myLR.gradientDescent()\n",
    "    myLR.lossfunction()\n",
    "    if ((i+1)%5 == 0):\n",
    "        print('epoch = %d, current loss = %.5f'%(i+1, myLR.loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "cb810917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38,)\n",
      "(38, 1)\n",
      "ACC is 0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "y_pred = myLR.predict(X_test_norm)\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "print('ACC is', Accuracy(y_pred,y_test.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ee657",
   "metadata": {},
   "source": [
    "This approach didn't seem to be working for some reason, and I am unfamiliar with object classes, so I decided to try it without those:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
